# -*- coding: utf-8 -*-
"""589 Final project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13-q01VweEnpAJ-R2ixq4y5SNQ95VrOeR
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC

datapath = './'
df_train = pd.read_csv(datapath + 'train.csv')
print(df_train.columns)
print(df_train.shape)
print(df_train.info())


df_test = pd.read_csv(datapath + 'test.csv')
print(df_test.shape)
print(df_test.info())

def drop_columns_with_missing_values(df, threshold):
    """
    Drop columns from the DataFrame that have missing values exceeding the specified threshold.

    Parameters:
    - df: pandas DataFrame
    - threshold: float, the maximum allowed percentage of missing values for a column

    Returns:
    - df: pandas DataFrame, the DataFrame with columns dropped
    """
    # Calculate the percentage of missing values for each column
    missing_percentage = df.isnull().mean()
    print('Missing % by columns ',missing_percentage)

    # Identify columns with missing values exceeding the threshold
    columns_to_drop = missing_percentage[missing_percentage > threshold].index

    print('Columns to drop ', columns_to_drop)

    # Drop columns with too many missing values
    df = df.drop(columns=columns_to_drop)

    return df

print('Train dataframe')
df_train = drop_columns_with_missing_values(df_train, threshold=0.5)
print('Test dataframe')
df_test = drop_columns_with_missing_values(df_test, threshold=0.5)

# assign PassengerId as ids for the dataframes
df_train.set_index("PassengerId", inplace=True)
df_test.set_index("PassengerId", inplace=True)

dfs = pd.concat([df_train, df_test], axis=0, sort=False)

dfs.info()

# impute NaN values of Age with the mean
dfs['Age'].fillna(dfs['Age'].mean(), inplace=True)

# impute NaN values of Embarked with the modt freqeuntly occuring value
dfs['Embarked'].fillna(dfs['Embarked'].mode()[0], inplace=True)

# impute NaN values of Fare with the mean
dfs['Fare'].fillna(dfs['Fare'].mean(), inplace=True)

"""# Feature Engineering

"""

# create a new column title from the Name column
dfs['Title'] = dfs.Name.str.extract('([A-Za-z]+)\.')
dfs['Title'] = dfs['Title'].replace({
    'Col': 'Military',
    'Major': 'Military',
    'Jonkheer': 'Noble',
    'Countess': 'Noble',
    'Lady': 'Noble',
    'Sir': 'Noble',
    'Don': 'Mr',
    'Dona': 'Mrs',
    'Mme': 'Mrs',
    'Mlle': 'Ms'
})

# create a new column FamilySize
dfs['FamilySize'] = dfs['SibSp'] + dfs['Parch'] + 1

# create new columns for the last name, is a minor and is a woman or a minor, mother and children count, is a mother
dfs['LastName'] = dfs.Name.str.split(',').str[0]
dfs['IsMinor'] = ((dfs.Title == 'Master')  | (dfs.Age <= 14)).astype(int)
dfs['IsWomanOrMinor'] = dfs.IsMinor | (dfs.Sex == 1).astype(int)
""" Counts mother and children per family (based on last name). It consideers
families with more than one member. Candidate mothers are identified by
the title> 'Mrs'"""
def motherChildrenCnt (row):
  out = 0
  if(((row['Title'] =='Mrs')  & (row['FamilySize'] > 1))):
    out = dfs[(dfs.IsMinor > 0) &
     (dfs.LastName == row.LastName)]['IsMinor'].sum() + 1
  if(((row['IsMinor'])  & (row['FamilySize'] > 1))):
    out = dfs[(dfs.IsMinor > 0) &
     (dfs.LastName == row.LastName)]['IsMinor'].sum() + 1
  return out;
dfs['MotherChildrenCnt'] = dfs.apply(lambda row: motherChildrenCnt(row), axis=1)
dfs['IsMother'] = ((dfs.MotherChildrenCnt > 0) & (dfs.Title == 'Mrs')).astype(int)

dfs.sample(10)

"""# Model Training

"""

colFeatures = ['Survived', 'Title', 'Pclass','Sex','IsWomanOrMinor','MotherChildrenCnt','Age']
numFeatures = dfs[colFeatures].select_dtypes(include='number').columns # numerical features
ordFeatures = pd.Index(['FamilySizeGr']) # ordinal features
# Categorical features removing ordinal features
catFeatures = dfs[colFeatures].select_dtypes('object').columns.difference(ordFeatures)
print('colFeatures',colFeatures)
print('numFeatures',numFeatures)
# print('ordFeatures',ordFeatures)
print('catFeatures',catFeatures)

# one hot encode the categorical data
catPip = Pipeline([('onehot', OneHotEncoder(handle_unknown = 'ignore', sparse_output=False))])
# Configured to include generated column names, using set_output function
col_trans = ColumnTransformer(transformers=[
    ('col_sel', 'passthrough', numFeatures),
    ('cat_pip', catPip, catFeatures)
],remainder='drop', verbose_feature_names_out=False, n_jobs=-1).set_output(transform='pandas')

dfs_trans = col_trans.fit_transform(dfs)

dfs_trans.sample(5)

# tree-based model (e.g., RandomForest) to visualize feature importances
from sklearn.ensemble import RandomForestClassifier

featImp = dfs_trans.loc[df_train.index]
featImpX = featImp.drop(['Survived'], axis=1)
featImpy = featImp['Survived']

# Fit a RandomForest model
rf_model_selection = RandomForestClassifier()
rf_model_selection.fit(featImpX, featImpy)

# Get feature importances
feature_importances = rf_model_selection.feature_importances_

# Create a bar plot for feature importances
plt.figure(figsize=(12, 6))
sns.barplot(x=feature_importances, y=featImpX.columns)
plt.title('Feature Importances')
plt.show()

from sklearn.model_selection import StratifiedShuffleSplit

# train-test split for the data
splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=77)
trainSet = dfs_trans.loc[df_train.index]
for train_idx, test_idx in splitter.split(trainSet,trainSet[['Survived']]):
    stratTrainSet = trainSet.iloc[train_idx]
    stratTestSet = trainSet.iloc[test_idx]


stratTrainSet.sample(5)

from sklearn.preprocessing import StandardScaler
X_train = stratTrainSet.drop('Survived', axis=1)
y_train = stratTrainSet['Survived']
X_test = stratTestSet.drop('Survived', axis=1)
y_test = stratTestSet['Survived']

# Standardize the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

testSet = dfs_trans.loc[df_test.index]
testSet.sample(5)

"""# SVM -  fixed-shape"""

# # hyperparameter tuning for SVM using rbf kernel
# from sklearn.model_selection import GridSearchCV
# from sklearn.svm import SVC

# C_range = np.logspace(-2, 10, 13)
# gamma_range = np.logspace(-9, 3, 13)
# print(C_range,gamma_range)

# # Define the parameter grid
# param_grid = {'C': C_range,'gamma': gamma_range}

# # Create an SVM model with an RBF kernel
# svm_model = SVC(kernel='rbf')

# # Perform Grid Search to find the best parameters
# grid_search = GridSearchCV(svm_model, param_grid, cv=5, n_jobs=-1)
# grid_search.fit(X_train_scaled, y_train)

# # Get the best parameters from the grid search
# best_params = grid_search.best_params_

# print(f'Best Parameters: {best_params}')

# Best Parameters: {'C': 100000.0, 'gamma': 1e-08} for SVM with rbf kernel

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report

# train the SVM model using the best parameters
svm_model = SVC(kernel='rbf', C=100000.0, gamma=1e-08)
svm_model.fit(X_train_scaled, y_train)

# Predictions on the test set
y_pred_svm = svm_model.predict(X_test_scaled)

# Performance Metrics
accuracy_svm = accuracy_score(y_test, y_pred_svm)
classification_report_svm = classification_report(y_test, y_pred_svm)


print(f'SVM Accuracy: {accuracy_svm}')
print(f'SVM Classification Report:\n{classification_report_svm}')

X_testcsv = testSet.drop('Survived', axis=1)
# Single column Df with testdf.index being that column, i.e. PassengerId
svm_result_df = pd.DataFrame(df_test.index)


X_test_scaledcsv = scaler.transform(X_testcsv)
svm_prediction = svm_model.predict(X_test_scaledcsv)

svm_result_df['Survived'] = svm_prediction.astype('int')
svm_result_df.to_csv('svm_submission.csv', index=False)

print(svm_prediction) # Score: 0.77033 on Kaggle

"""# Tree"""

# hyperparamter tuning for RandomForestClassifier
# # Define the parameter grid
# from sklearn.ensemble import RandomForestClassifier
# # Define the parameter grid
# param_grid_rf = {
#     'n_estimators': [50, 100, 150],
#     'max_depth': [None, 10, 20, 30],
#     'min_samples_split': [2, 5, 10],
#     'min_samples_leaf': [1, 2, 4],
#     'max_features': ['auto', 'sqrt', 'log2'],
#     'bootstrap': [True, False]
# }

# # Create a RandomForestClassifier
# rf_model = RandomForestClassifier(random_state=42)

# # Perform Grid Search to find the best parameters
# grid_search = GridSearchCV(rf_model, param_grid_rf, cv=5, n_jobs=-1)
# grid_search.fit(X_train_scaled, y_train)

# # Get the best parameters from the grid search
# best_params = grid_search.best_params_


# print(f'Best Parameters: {best_params}')

from sklearn.ensemble import RandomForestClassifier

# Best parameters obtained from the grid search
rf_best_params = {
    'bootstrap': False,
    'max_depth': None,
    'max_features': 'auto',
    'min_samples_leaf': 4,
    'min_samples_split': 10,
    'n_estimators': 50
}

# Initialize and train the Random Forest model
rf_model = RandomForestClassifier(**rf_best_params)
rf_model.fit(X_train_scaled, y_train)

# Predictions on the test set
y_pred_rf = rf_model.predict(X_test_scaled)
# Performance Metrics
accuracy_rf = accuracy_score(y_test, y_pred_rf)
classification_report_rf = classification_report(y_test, y_pred_rf)

print(f'Random Forest Accuracy: {accuracy_rf}')
print(f'Random Forest Classification Report:\n{classification_report_rf}')

rf_result_df = pd.DataFrame(df_test.index)
rf_prediction = rf_model.predict(X_test_scaledcsv)

rf_result_df['Survived'] = rf_prediction.astype('int')
rf_result_df.to_csv('rf_submission.csv', index=False)

print(rf_prediction)

"""# NN - MLP Classifier"""

# hyperparameter tuning for MLPClassifier
# from sklearn.model_selection import train_test_split, GridSearchCV
# from sklearn.neural_network import MLPClassifier
# # Define the parameter grid
# param_grid_mlp = {
#     'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],
#     'activation': ['relu', 'tanh', 'logistic'],
#     'solver': ['sgd', 'adam'],
#     'max_iter': [200, 500, 1000],
# }

# # Create an MLPClassifier
# mlp_model = MLPClassifier(random_state=42)

# # Perform Grid Search to find the best parameters
# grid_search = GridSearchCV(mlp_model, param_grid_mlp, cv=5, n_jobs=-1)
# grid_search.fit(X_train_scaled, y_train)

# # Get the best parameters from the grid search
# best_params = grid_search.best_params_

# print(f'Best Parameters: {best_params}')

from sklearn.neural_network import MLPClassifier
from sklearn.pipeline import make_pipeline

best_mlp_params = {
    'activation': 'tanh',
    'hidden_layer_sizes': (100,),
    'max_iter': 500,
    'solver': 'adam'
}


# Initialize and train the MLP model
mlp_model = MLPClassifier(**best_mlp_params)
mlp_model.fit(X_train_scaled, y_train)

# Predictions on the test set
y_pred_mlp = mlp_model.predict(X_test_scaled)

# Performance Metrics
accuracy_mlp = accuracy_score(y_test, y_pred_mlp)
classification_report_mlp = classification_report(y_test, y_pred_mlp)

print(f'MLP Accuracy: {accuracy_mlp}')
print(f'MLP Classification Report:\n{classification_report_mlp}')

# Best Parameters: {'activation': 'tanh', 'hidden_layer_sizes': (100,), 'max_iter': 500, 'solver': 'adam'}
# MLP Accuracy: 0.7932960893854749

mlp_result_df = pd.DataFrame(df_test.index)
mlp_prediction = mlp_model.predict(X_test_scaledcsv)

mlp_result_df['Survived'] = mlp_prediction.astype('int')
mlp_result_df.to_csv('mlp_submission.csv', index=False)

print(mlp_prediction)

# MLP Kaggle : Score: 0.77033

"""# Comparing Model Results


"""

import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

# Assuming y_test and predictions for each model
cm_svm = confusion_matrix(y_test, y_pred_svm)
cm_rf = confusion_matrix(y_test, y_pred_rf)
cm_mlp = confusion_matrix(y_test, y_pred_mlp)

# Plot confusion matrix heatmaps
plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
sns.heatmap(cm_svm, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('SVM Confusion Matrix')

plt.subplot(1, 3, 2)
sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Random Forest Confusion Matrix')

plt.subplot(1, 3, 3)
sns.heatmap(cm_mlp, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('MLP Confusion Matrix')

plt.show()

import pandas as pd
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score

# Assuming y_test and predictions for each model
models = ['SVM', 'Random Forest', 'MLP']
predictions = [y_pred_svm, y_pred_rf, y_pred_mlp]

# Initialize an empty DataFrame to store the results
results_df = pd.DataFrame(index=models, columns=['Accuracy', 'Precision', 'Recall', 'F1 Score', 'AUC-ROC', 'AUC-PR'])

# Populate the DataFrame with performance metrics
for model, y_pred in zip(models, predictions):
    results_df.loc[model, 'Accuracy'] = accuracy_score(y_test, y_pred)
    results_df.loc[model, 'Precision'] = precision_score(y_test, y_pred)
    results_df.loc[model, 'Recall'] = recall_score(y_test, y_pred)
    results_df.loc[model, 'F1 Score'] = f1_score(y_test, y_pred)
    results_df.loc[model, 'AUC-ROC'] = roc_auc_score(y_test, y_pred)
    results_df.loc[model, 'AUC-PR'] = average_precision_score(y_test, y_pred)

# Display the results table
print(results_df)